<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0//EN">
<html>
  <head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
  <meta name="referrer" content="unsafe-url">    
  <title>Hsuan-Tien Lin > Courses > Machine Learning, Fall 2023</title>
  <style type="text/css">
    BODY {background:#dfd; font-family:sans-serif}
    DIV.center {text-align:center}
  </style>
  </head>
  
  <body>

<h2>Machine Learning, Fall 2023 (Temp Page)</h2>

<h3>Course Description</h3>
<p>Machine learning allows computational systems to adaptively improve their performance with experience accumulated from the data observed. This course introduces the basics of learning theories, the design and analysis of learning algorithms, and some applications of machine learning.

<h3>People</h3>

<ul>
	  <li>instructor: <a href="http://www.csie.ntu.edu.tw/~htlin">Hsuan-Tien LIN</a> (htlin AT csie . ntu . edu . tw) [office hour: after classes, or by appointment]
<li>TAs and TA hour: html_ta AT csie . ntu . edu . tw
  <ul>
    <li> Chia-Wei CHANG (Undergraduate in CSIE Department)
    <li> Yu-Cheng CHENG (M.S. Student in CSIE Department)
    <li> Shuo-Chen HO (Undergraduate in CSIE Department)
    <li> Cai-Yi HU (M.S. Student in CSIE Department)
    <li> Yu-Shiang HUANG (Ph.D. Student in Data Science Program)
    <li> Ren-Wei (Willy) LIANG (Undergraduate in CSIE Department)
    <li> Jeng-Yue (Buffett) LIU (Undergraduate in Geography Department)
    <li> Poy LU (Ph.D. student in Graduate Institute of Networking and Multimedia)
    <li> Odo To (Undergraduate in CSIE Department)
    <li> Cheng-Chi (Casper) WANG (Undergraduate in CSIE Department)
    <li> Hsuan-Fu WANG (M.S. Student in Graduate Institute of Networking and Multimedia)
  </ul>
</ul>
<h3>Course Information</h3>
<ul>

<li>Time: Wednesdays 9:10 to 12:10
<li>Room: CSIE R104 (with broadcast to R102)
<li>NTU COOL: <a href="https://cool.ntu.edu.tw/courses/32477">https://cool.ntu.edu.tw/courses/32477</a>
<li>Slido: #HTML2023FALL
<li>Textbook: <a href="http://amlbook.com"><i>Learning from Data</i></a>, by Yaser Abu-Mostafa, Malik Magdon-Ismail and Hsuan-Tien Lin
<li>Language: <b>Mandarin</b> teaching
<li>Grading: 70% homework, 30% project (tentative)
</ul>

<h3>Announcements</h3>
<ul>
  <li>2023/09/09: homework 1 announced <a href="hw1">here</a>, due on 2023/09/27
  <li>2023/09/09: homework 0 announced <a href="doc/hw0.pdf">here</a>, due on 2023/09/27
  <li>2023/08/31: signup form announced <a href="https://forms.gle/sMFK6q2gciq4oYbX6">here</a>
  <li>2023/08/31: course policy announced <a href="doc/policy.pdf">here</a>
</ul>

<h3>Class Policy</h3>
<ul>
  <li><a href="doc/policy.pdf">policy</a>
</ul>

<h3>Course Plan (tentative)</h3>

<TABLE border=1>
  <TR><TD>date</TD><TD>syllabus</TD><TD>todo/done</TD><TD>materials</TD></TR>
  <TR>
    <TD>09/06 (W1)</TD>
    <TD>
      course introduction;<br>
      <b>topic 1: when can machines learn?</b><br>
      the learning problem<br>      
    </TD>
    <TD></TD>
    <TD>
      <ul>
	<li>course introduction <a href="doc/00_handout.pdf">course slides</a>;<br>
	<li>the learning problem <a href="doc/01_handout.pdf">course slides</a> and <a href="doc/01e_handout.pdf">extended slides</a>; LFD 1.0, 1.1.1, 1.2.4
      </ul>
    </TD>
  </TR>
  <TR>
    <TD>09/13 (W2)</TD>
    <TD>
      learning to answer yes/no;<br>
      types of learning      
    </TD>
    <TD>homework 1 announced</TD>
    <TD>
      <ul>
	<li><a href="doc/02e_handout.pdf">Lecture 2 extended slides</a>
	<li><a href="doc/03e_handout.pdf">Lecture 3 extended slides</a>
      </ul>
      <b>required</b> watching (before class):
      <ul>
	<li><a href="doc/02_handout.pdf">course slides</a>; LFD 1.1.2, 3.1
	<li><a href="https://youtu.be/WlpF1Phkv28">Learning to Answer Yes/No :: Perceptron Hypothesis Set</a>
	<li><a href="https://youtu.be/1xnUlrgJJGo">Learning to Answer Yes/No :: Perceptron Learning Algorithm</a>
	<li><a href="https://youtu.be/Okrrz0IYoSE">Learning to Answer Yes/No :: Guarantee of PLA</a>
	<li><a href="https://youtu.be/vT-mUeRJfys">Learning to Answer Yes/No :: Non-Separable Data</a>
      </ul>      
      <b>required</b> watching (before class):
      <ul>
	<li><a href="doc/03_handout.pdf">course slides</a>; LFD 1.2; LFD 1.3
	<li><a href="https://youtu.be/XyJQm4mvVUA">Types of Learning :: Learning with Different Output Space</a>
	<li><a href="https://youtu.be/8w1lDGaoFTI">Types of Learning :: Learning with Different Data Label</a>
	<li><a href="https://youtu.be/emLW7jLh-n0">Types of Learning :: Learning with Different Protocol</a>
	<li><a href="https://youtu.be/cIsgI7tktQM">Types of Learning :: Learning with Different Input Space</a>
      </ul>      
    </TD>
  </TR>   
  <TR>
    <TD>09/20 (W3)</TD>
    <TD>
      feasibility of learning;<br>
      <b>topic 2: why can machines learn?</b><br>
      training versus testing      
    </TD>
    <TD></TD>
    <TD>
      <ul>
	<li><a href="doc/04e_handout.pdf">Lecture 4 extended slides</a>
	<li>no Lecture 5 extended slides---it's heavy enough ;-)
      </ul>
      <b>required</b> watching (before class):
      <ul>
	<li><a href="doc/04_handout.pdf">course slides</a>; LFD 1.3
	<li><a href="https://youtu.be/tOgbh5_747w">Feasibility of Learning :: Learning is Impossible?</a>
	<li><a href="https://youtu.be/MgAihqFPkZc">Feasibility of Learning :: Probability to the Rescue</a>
	<li><a href="https://youtu.be/iXbbfjJNfwU">Feasibility of Learning :: Connection to Learning</a>
	<li><a href="https://youtu.be/MFL6xDn1lXM">Feasibility of Learning :: Connection to Real Learning</a>	
      </ul>
      <b>required</b> watching (before class):
      <ul>
	<li><a href="doc/05_handout.pdf">course slides</a>; LFD 2.0, 2.1.1
	<li><a href="https://youtu.be/4aIAxH8eBMs">Training versus Testing :: Recap and Preview</a>
	<li><a href="https://youtu.be/oAW0_j8_l3Y">Training versus Testing :: Effective Number of Lines</a>
	<li><a href="https://youtu.be/dnVofdAomWY">Training versus Testing :: Effective Number of Hypotheses</a>
	<li><a href="https://youtu.be/z3TpJRqPzcg">Training versus Testing :: Break Point</a>	
      </ul>
      suggested extended reading: 
      <ul>
	<li><a href="https://direct.mit.edu/neco/article-abstract/8/7/1341/6016/The-Lack-of-A-Priori-Distinctions-Between-Learning">The Lack of A Priori Distinctions Between Learning Algorithms (Wolpert)</a>
	</li>	
    </TD>
  </TR>
  <TR>
    <TD>09/27 (W4)</TD>
    <TD>
      the VC dimension
    </TD>
    <TD>homework 1 due; homework 2 announced</TD>
    <TD>
      suggested watching (anytime):
      <ul>
	<li><a href="doc/06_handout.pdf">course slides</a>; LFD 2.0, 2.1.1
	<li><a href="https://youtu.be/rUFqB5Z3YHQ">Theory of Generalization :: Restriction of Break Point</a>
	<li><a href="https://youtu.be/OmRekto9rkc">Theory of Generalization :: Bounding Function: Basic Cases</a>
	<li><a href="https://youtu.be/6jtWUmaBqFU">Theory of Generalization :: Bounding Funciton: Inductive Cases</a>
	<li><a href="https://youtu.be/GcxpsIvR7t8">Theory of Generalization :: A Pictorial Proof</a>
      </ul>
      <b>required</b> watching (before class):
      <ul>
	<li><a href="doc/07_handout.pdf">course slides</a>; LFD 2.2
	<li><a href="https://youtu.be/XxPB9GlJEUk">The VC Dimension :: Definition of VC Dimension</a>
	<li><a href="https://youtu.be/WQzhc1IdB_I">The VC Dimension :: VC Dimension of Perceptrons</a>
	<li><a href="https://youtu.be/5-V5WCf8cY8">The VC Dimension :: Physical Intuition of VC Dimension</a>
	<li><a href="https://youtu.be/_DN_oF-i6ag">The VC Dimension :: Interpreting VC Dimension</a>
      </ul>      
    </TD>  
  </TR>  
  <TR>
    <TD>10/04 (W5)</TD>
    <TD>
      noise and error;<br>
      <b>topic 3: how can machines learn?</b><br>
      linear regression;<br>
      logistic regression
    </TD>
    <TD></TD>
    <TD>
      <ul>
	<li><a href="doc/09e_handout.pdf">Lecture 9 extended slides</a>
      </ul>
      <b>required</b> watching (before class):
      <ul>
	<li><a href="doc/08_handout.pdf">course slides</a>; LFD 1.4
	<li><a href="https://youtu.be/Br8J5pZM_CE">Noise and Error :: Noise and Probabilistic Target</a>
	<li><a href="https://youtu.be/2gCnX0V1do8">Noise and Error :: Error Measure</a>
	<li><a href="https://youtu.be/0ApgGq4mh1E">Noise and Error :: Algorithmic Error Measure</a>
	<li><a href="https://youtu.be/XfuRb1jT4hs">Noise and Error :: Weighted Classification</a>
      </ul>      
      <b>required</b> watching (before class):
      <ul>
	<li><a href="doc/09_handout.pdf">course slides</a>; LFD 3.2
	<li><a href="https://youtu.be/qGzjYrLV-4Y">Linear Regression :: Linear Regression Problem</a>
	<li><a href="https://youtu.be/2LfdSCdcg1g">Linear Regression :: Linear Regression Algorithm</a>
	<li><a href="https://youtu.be/lj2jK1FSwgo">Linear Regression :: Generalization Issue</a>
	<li><a href="https://youtu.be/tF1HTirYbtc">Linear Regression :: for Binary Classification</a>
      </ul>
      <b>required</b> watching (before class):
      <ul>
	<li><a href="doc/10_handout.pdf">course slides</a>; LFD 3.3
	<li><a href="https://youtu.be/4rPupwSdAac">Logistic Regression :: Logistic Regression Problem</a>
      </ul>
    </TD>
  </TR>  
  <TR>
    <TD>10/11 (W6)</TD>
    <TD>
      linear models for classification;<br>
      nonlinear transformation
    </TD>
    <TD>homework 3 announced</TD>
    <TD>
      <b>required</b> watching (before class):
      <ul>
	<li><a href="doc/10_handout.pdf">course slides</a>; LFD 3.3
	<li><a href="https://youtu.be/Uw62i3-Tr4Q">Logistic Regression :: Logistic Regression Error</a>
	<li><a href="https://youtu.be/IZttt_v5tSw">Logistic Regression :: Gradient of Logistic Regression Error</a>
	<li><a href="https://youtu.be/X9NTihvSdjw">Logistic Regression :: Gradient Descent</a>
      </ul>
      <b>required</b> watching (before class):
      <ul>
	<li><a href="doc/11_handout.pdf">course slides</a>; LFD 3.3 (for SGD part only)
	<li><a href="https://youtu.be/qXfDVHVzI38">Linear Models for Classification :: Binary Classification</a>
	<li><a href="https://youtu.be/9HL3YvmrovQ">Linear Models for Classification :: Stochastic Gradient Descent</a>
	<li><a href="https://youtu.be/wnM435PDHGY">Linear Models for Classification :: Multiclass via Logistic</a>
	<li><a href="https://youtu.be/vxnjOI_ASlw">Linear Models for Classification :: Multiclass via Binary</a>
      </ul>
      <b>required</b> watching (before class):
      <ul>
	<li><a href="doc/12_handout.pdf">course slides</a>; LFD 3.4
	<li><a href="https://youtu.be/8pQ06pku1xA">Nonlinear Transformation :: Quadratic Hypotheses</a>
	<li><a href="https://youtu.be/UHAn6Cuk8zk">Nonlinear Transformation :: Nonlinear Transform</a>
      </ul>
    </TD>
  </TR>
  <TR>
    <TD>10/18 (W7)</TD>
    <TD>
      nonlinear transformation; <br>
      <b>topic 4: how can machines learn better?</b><br>
      hazard of overfitting;<br>
      regularization
    </TD>
    <TD>homework 2 due</TD>
    <TD>
      <b>required</b> watching (before class):
      <ul>
	<li><a href="doc/12_handout.pdf">course slides</a>; LFD 3.4
	<li><a href="https://youtu.be/Inxr-Yc1Aow">Nonlinear Transformation :: Price of Nonlinear Transform</a>
	<li><a href="https://youtu.be/gcLmU3MC3bE">Nonlinear Transformation :: Structured Hypothesis Sets</a>
      </ul>
      <b>required</b> watching (before class):
      <ul>
	<li><a href="doc/13_handout.pdf">course slides</a>; LFD 4.0, 4.1
	<li><a href="https://youtu.be/BA76U3JBDdE">Hazard of Overfitting :: What is Overfitting?</a>
	<li><a href="https://youtu.be/6bfcLhHhgs0">Hazard of Overfitting :: The Role of Noise and Data Size</a>
	<li><a href="https://youtu.be/c_208kUQEis">Hazard of Overfitting :: Deterministic Noise</a>
	<li><a href="https://youtu.be/r3bX1k7tcjc">Hazard of Overfitting :: Dealing with Overfitting</a>
      </ul>
      <b>required</b> watching (before class):
      <ul>
	<li><a href="doc/14_handout.pdf">course slides</a>; LFD 4.2
	<li><a href="https://youtu.be/Sno7I5slFUA">Regularization :: Regularized Hypothesis Set</a>
	<li><a href="https://youtu.be/idWnPdW9znM">Regularization :: Weight Decay Regularization</a>
	<li><a href="https://youtu.be/15JB2o4VUeY">Regularization :: Regularization and VC Theory</a>
	<li><a href="https://youtu.be/PeQeKeeGu3A">Regularization :: General Regularizers</a>
      </ul>
    </TD>
  </TR>
  <TR>
    <TD>10/25 (W8)</TD>
    <TD>
      validation;<br>
      three learning principles
    </TD>
    <TD>final project announced</TD>
    <TD>
      <b>required</b> watching (before class):      
      <ul>
	<li><a href="doc/15_handout.pdf">course slides</a>; LFD 4.3
	<li><a href="https://youtu.be/BRLGPnrcel8">Validation :: Model Selection Problem</a>
	<li><a href="https://youtu.be/RvkCaAwRP8A">Validation :: Validation</a>
	<li><a href="https://youtu.be/iToz5t0J6WU">Validation :: Leave-One-Out Cross Validation</a>
	<li><a href="https://youtu.be/Y8PaLsYm0Ac">Validation :: V-Fold Cross Validation</a>
      </ul>
      <b>required</b> watching (before class):
      <ul>
	<li><a href="doc/16_handout.pdf">course slides</a>; LFD 5
	<li><a href="https://youtu.be/Oj6j98ceUz8">Three Learning Principles :: Occam's Razor</a>
	<li><a href="https://youtu.be/8QZZiIAdTUU">Three Learning Principles :: Sampling Bias</a>
	<li><a href="https://youtu.be/7nP5zWMQmxM">Three Learning Principles :: Data Snooping</a>
	<li><a href="https://youtu.be/29jgHPeRAqI">Three Learning Principles :: Power of Three</a>
      </ul>
    </TD>
  </TR>  
  <TR>
    <TD>11/01 (W9)</TD>
    <TD>
      <b>topic 5: how can machines learn by embedding numerous features?</b><br>
      linear support vector machine;<br>
      dual support vector machine
    </TD>
    <TD>homework 3 due; homework 4 announced</TD>
    <TD>
      <b>required</b> watching (before class):
      <ul>
	<li><a href="doc/201_handout.pdf">course slides</a>; LFD e-8.1
	<li><a href="https://youtu.be/8hak0XngnV0">Linear SVM :: Large-Margin Separating Hyperplane</a>
	<li><a href="https://youtu.be/lHo9GcIURRs">Linear SVM :: Standard Large-Margin Problem</a>
	<li><a href="https://youtu.be/FAm70y081o4">Linear SVM :: Support Vector Machine</a>
	<li><a href="https://youtu.be/7UUO_AamxcA">Linear SVM :: Reasons behind Large-Margin Hyperplane</a>
      </ul>
      <b>required</b> watching (before class):
      <ul>
	<li><a href="doc/202_handout.pdf">course slides</a>; LFD e-8.2
	<li><a href="https://youtu.be/VUp-17l03lk">Dual Support Vector Machine :: Motivation of Dual SVM</a>
	<li><a href="https://youtu.be/Yhwtvbzg9Fw">Dual Support Vector Machine :: Largange Dual SVM</a>
	<li><a href="https://youtu.be/qGk0p7K07Mc">Dual Support Vector Machine :: Solving Dual SVM</a>
	<li><a href="https://youtu.be/agmmQh702aA">Dual Support Vector Machine :: Messages behind Dual SVM</a>
      </ul>      
    </TD>
  </TR>
  <TR>
    <TD>11/08 (W10)</TD>
    <TD>
      kernel support vector machine;<br>
      soft-margin support vector machine
    </TD>
    <TD></TD>
    <TD>
      <b>required</b> watching (before class):
      <ul>
	<li><a href="doc/203_handout.pdf">course slides</a>; LFD e-8.3
	<li><a href="https://youtu.be/oOi7kqUTqxw">Kernel Support Vector Machine :: Kernel Trick</a>
	<li><a href="https://youtu.be/Fb-WSBvsPak">Kernel Support Vector Machine :: Polynomial Kernel</a>
	<li><a href="https://youtu.be/_-fIkbSBdF8">Kernel Support Vector Machine :: Gaussian Kernel</a>
	<li><a href="https://youtu.be/sacJmcs8TKE">Kernel Support Vector Machine :: Comparison of Kernels</a>
      </ul>
      <b>required</b> watching (before class):
      <ul>
	<li><a href="doc/204_handout.pdf">course slides</a>; LFD e-8.4
	<li><a href="https://youtu.be/K7ZcAYXuU_A">Soft-Margin Support Vector Machine :: Motivation and Primal</a>
	<li><a href="https://youtu.be/fTHTqW5Uq4U">Soft-Margin Support Vector Machine :: Dual Problem</a>
	<li><a href="https://youtu.be/5z7ujI3YBBE">Soft-Margin Support Vector Machine :: Messages</a>
	<li><a href="https://youtu.be/ahogAa5Rnmc">Soft-Margin Support Vector Machine :: Model Selection</a>
      <ul>
    </TD>
  </TR>
  <TR>
    <TD>11/15 (W11)</TD>
    <TD>
      kernel logistic regression;<br>
      <b>topic 6: how can machines learn by combining predictive features?</b><br>
      blending and bagging
    </TD>
    <TD>homework 4 due; homework 5 announced</TD>
    <TD>
      <b>required</b> watching (before class):
      <ul>
	<li><a href="doc/205_handout.pdf">course slides</a>
	<li><a href="https://youtu.be/Bc8bg5ZkRdk">Kernel Logistic Regression :: Soft-Margin SVM as Regularized Model</a>
	<li><a href="https://youtu.be/5K44AgZvcDk">Kernel Logistic Regression :: SVM versus Logistic Regression</a>
	<li><a href="https://youtu.be/pNfvZYH5iFg">Kernel Logistic Regression :: SVM for Soft Binary</a>
	<li><a href="https://youtu.be/AbaIkcQUQuo">Kernel Logistic Regression :: Kernel Logistic Regression</a>
      </ul>      
      <b>required</b> watching (before class):
      <ul>
	<li><a href="doc/207_handout.pdf">course slides</a>
	<li><a href="https://youtu.be/mjUKsp0MvMI">Blending and Bagging :: Motivation of Aggregation</a>
	<li><a href="https://youtu.be/DAFkKJYTMW4">Blending and Bagging :: Uniform Blending</a>
	<li><a href="https://youtu.be/i03s1g7X_m4">Blending and Bagging :: Linear and Any Blending</a>
	<li><a href="https://youtu.be/3T1mdvzRAF0">Blending and Bagging :: Bagging (Bootstrap Aggregation)</a>
      </ul>
      suggested extended reading: 
      <ul>
	<li><a href="http://papers.nips.cc/paper/2059-kernel-logistic-regression-and-the-import-vector-machine.pdf">Kernel Logistic Regression and the Import Vector Machine (Zhu and Hastie)</a>
	<li><a href="http://www.csie.ntu.edu.tw/~htlin/paper/doc/plattprob.pdf">A Note on Platt's Probabilistic Outputs for Support Vector Machines (Lin, Weng and Lin)</a>
	<li><a href="http://www.csie.ntu.edu.tw/~htlin/paper/doc/wskdd11cup_one.pdf">A linear ensemble of individual and blended models for music rating prediction (Chen et al.)</a>
	<li><a href="http://statistics.berkeley.edu/sites/default/files/tech-reports/421.pdf">Bagging predictors (Breiman)</a>
      </ul>
    </TD>
  <TR>
    <TD>11/22 (W12)</TD>
    <TD>
      adaptive boosting;<br>
      decision tree<br>
    </TD>
    <TD>
    </TD>
    <TD>
      <b>required</b> watching (before class):
      <ul>
	<li><a href="doc/208_handout.pdf">course slides</a>
	<li><a href="https://youtu.be/hL8DjIHAzZY">Adaptive Boosting :: Motivation of Boosting</a>
	<li><a href="https://youtu.be/pTNKUj_1Dw8">Adaptive Boosting :: Diversity by Re-weighting</a>
	<li><a href="https://youtu.be/vqTXLTYqbbw">Adaptive Boosting :: Adaptive Boosting Algorithm</a>
	<li><a href="https://youtu.be/5wPN87bwoaE">Adaptive Boosting :: Adaptive Boosting in Action</a>
      </ul>
      <b>required</b> watching (before class):
      <ul>
	<li><a href="doc/209_handout.pdf">course slides</a>
	<li><a href="https://youtu.be/dAqPpAXnMJ4">Decision Tree :: Decision Tree Hypothesis</a>
	<li><a href="https://youtu.be/s9Um2O7N7YM">Decision Tree :: Decision Tree Algorithm</a>
	<li><a href="https://youtu.be/uvGC_Y0EYiA">Decision Tree :: Decision Tree Heuristics in CART</a>
	<li><a href="https://youtu.be/ryWTrPPbqcg">Decision Tree :: Decision Tree in Action</a>
      </ul>
      suggested extended reading: 
      <ul>
	<li><a href="http://cseweb.ucsd.edu/~yfreund/papers/IntroToBoosting.pdf">A short introduction to boosting (Freund and Schapire)</a>
	<li><a href="http://www.stat.wisc.edu/~loh/treeprogs/guide/wires11.pdf">Classification and regression trees (overview of decision tree by Loh)</a>
	<li><a href="http://www.amazon.com/Classification-Regression-Wadsworth-Statistics-Probability/dp/0412048418">Classification and regression trees (book of CART by Breiman et al.)</a>
      </ul>
    </TD>
  </TR>
  <TR>
    <TD>11/29 (W13)</TD>
    <TD>
      random forest;<br>      
      gradient boosted decision tree<br>
    </TD>
    <TD>
      homework 5 due; homework 6 announced
    </TD>
    <TD>
      <b>required</b> watching (before class):
      <ul>      
	<li><a href="doc/210_handout.pdf">course slides</a>
	<li><a href="https://youtu.be/ATM3sH0D45s">Random Forest :: Random Forest Algorithm</a>
	<li><a href="https://youtu.be/7oz5aO-FkR0">Random Forest :: Out-of-bag Estimate</a>
	<li><a href="https://youtu.be/ChqNC94JXtM">Random Forest :: Feature Selection</a>
	<li><a href="https://youtu.be/Ipfpf7AW_yM">Random Forest :: Random Forest in Action</a>
      </ul>
      <b>required</b> watching (before class):
      <ul>
	<li><a href="doc/211_handout.pdf">course slides</a>
	<li><a href="https://youtu.be/aX6ZiIWLjdk">Gradient Boosted Decision Tree :: Adaptive Boosted Decision Tree</a>
	<li><a href="https://youtu.be/lKkXrFVcZjs">Gradient Boosted Decision Tree :: Optimization of AdaBoost</a>
	<li><a href="https://youtu.be/F_EuNXhS9js">Gradient Boosted Decision Tree :: Gradient Boosting</a>
	<li><a href="https://youtu.be/JqSLmlSpqNo">Gradient Boosted Decision Tree :: Summary of Aggregation</a>
      </ul>
      suggested extended reading: 
      <ul>
	<li><a href="http://dx.doi.org/10.1023%2FA%3A1010933404324">Random forest (Breiman)</a>
	<li><a href="http://www-stat.stanford.edu/~jhf/ftp/trebst.pdf">Greedy Function Approximation: A Gradient Boosting Machine (Friedman)</a>	  
      </ul>
      extended reading:       
      <ul>
	<li><a href="https://ieeexplore.ieee.org/document/5197422">Matrix Factorization Techniques for Recommender Systems (Koren, Bell and Folinsky)</a>
      </ul>      
    </TD>
  </TR>
  <TR>
    <TD>12/06 (W14)</TD>
    <TD>
      <b>topic 7: how can machines learn by distilling hidden features?</b><br>
      neural network;<br>
      (preliminary) deep learning
    </TD>
    <TD></TD>
    <TD>
      <b>required</b> watching (before class):
      <ul>
	<li><a href="doc/212_handout.pdf">course slides</a>; LFD e-7.1, e-7.2, e-7.3, e-7.4 (selected parts)
	<li><a href="https://youtu.be/GwRS2YJv2Ck">Neural Network :: Motivation</a>
	<li><a href="https://youtu.be/giOcWMbi1bU">Neural Network :: Neural Network Hypothesis</a>
	<li><a href="https://youtu.be/Z26n4YGNWvQ">Neural Network :: Neural Network Learning</a>
	<li><a href="https://youtu.be/z2tHzMzoOOs">Neural Network :: Optimization and Regularization</a>
      </ul>
      <b>required</b> watching (before class):
      <ul>
	<li><a href="doc/213_handout.pdf">course slides</a>; LFD e-7.6
	<li><a href="https://youtu.be/H1czfox0Nog">Deep Learning :: Deep Neural Network</a>
	<li><a href="https://youtu.be/eBVPQ4fgs_k">Deep Learning :: Autoencoder</a>
	<li><a href="https://youtu.be/gx2Vfw8S--0">Deep Learning ::Denoising Autoencoder</a>
	<li><a href="https://youtu.be/Bgc4UY8567A">Deep Learning :: Principal Component Analysis</a>
      </ul>
    </TD>
  </TR>
  <TR>
    <TD>12/13 (W15)</TD>
    <TD>
      radial basis function network;<br>
      matrix factorization;<br>
      <b>no class as instructor needs to attend NeurIPS 2023</b></TD>
    <TD>
      homework 6 due
    </TD>
    <TD>
      suggested watching:
      <ul>
	<li><a href="doc/214_handout.pdf">course slides</a>
	<li><a href="https://youtu.be/7lHhnpdPVr0">Radial Basis Function Network :: RBF Network Hypothesis</a>
	<li><a href="https://youtu.be/dEYdx2rS66c">Radial Basis Function Network :: RBF Network Learning</a>
	<li><a href="https://youtu.be/ker9RF2TDUU">Radial Basis Function Network :: k-Means Algorithm</a>
	<li><a href="https://youtu.be/D5elADTz1vk">Radial Basis Function Network :: k-Means and RBFNet in Action</a>
      </ul>            
      suggested watching:
      <ul>
	<li><a href="doc/215_handout.pdf">course slides</a>
	<li><a href="https://youtu.be/2pX76iH_irw">Matrix Factorization :: Linear Network Hypothesis</a>
	<li><a href="https://youtu.be/3l5kaWkcR6s">Matrix Factorization :: Basic Matrix Factorization</a>
	<li><a href="https://youtu.be/br3IzOz-xMs">Matrix Factorization :: Stochastic Gradient Descent</a>
	<li><a href="https://youtu.be/xKZMB4T2a2s">Matrix Factorization :: Summary of Extraction Models</a>
      </ul>      
  </TR>
  <TR>
    <TD>12/20 (W16)</TD>
    <TD><br>
      modern deep learning (tentative)<br>
      finale and award ceremony
    </TD>
    <TD></TD>
    <TD>
      <ul>
	<li>ReLU: <a href="http://jmlr.org/proceedings/papers/v15/glorot11a/glorot11a.pdf">Deep sparse rectifier neural networks (Glorot, Bordes and Bengio)</a>
	<li>leaky ReLU: <a href="https://ai.stanford.edu/~amaas/papers/relu_hybrid_icml2013_final.pdf">Rectifier Nonlinearities Improve Neural Network Acoustic Models (Maas, Hannun and Ng)</a>
	<li>parametric ReLU: <a href="https://arxiv.org/abs/1502.01852">Delving Deep into Rectifiers: Surpassing Human-Level Performance on Image Net Classification (He, Zhang, Ren and Sun)</a>
	<li><a href="http://d2l.ai/">Dive into Deep Learning</a> Section 4.1.2
	<li>Gloret initialization: <a href="http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf">Understanding the difficulty of training deep feedforward neural networks (Gloret and Bengio)</a>
	<li>He initialization: <a href="https://arxiv.org/pdf/1502.01852.pdf">Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification (He et al.)</a>
	<li>backprop and momentum: 
	  <a href="https://rdcu.be/b4ocH">Learning representations by back-propagating errors (Rumelhart, Hinton, and Williams)</a>;
	  <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.57.5612&amp;rep=rep1&amp;type=pdf">On the Momentum Term in Gradient Descent Learning Algorithms (Qian)</a>
	<li>adam: <a href="https://arxiv.org/pdf/1412.6980.pdf">Adam: A Method for Stochastic Optimization (Kingma and Ba)</a>
	<li><a href="http://d2l.ai/">Dive into Deep Learning</a> Sections 4.8, 11.6, 11.8, 11.10
	<li><a href="http://jmlr.org/papers/v15/srivastava14a.html">Dropout: A Simple Way to Prevent Neural Networks from Overfitting (Srivastava, Hinton, Krizhevsky, Sutskever and Salakhutdinov)</a>
	<li><a href="http://d2l.ai/">Dive into Deep Learning</a> Sections 4.6
      </ul>      
    </TD>
  </TR>  
  <TR>
    <TD>12/27 (W17)</TD>
    <TD><b>no class and winter vacation started (really?)</b></TD>
    <TD>final project due</TD>
    <TD></TD>
  </TR>
</TABLE>

</html>
