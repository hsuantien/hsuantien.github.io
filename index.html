<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0//EN">
<html>
  <head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
  <meta name="referrer" content="unsafe-url">    
  <title>Hsuan-Tien Lin > Courses > Machine Learning, Fall 2024</title>
  <style type="text/css">
    BODY {background:#dfd; font-family:sans-serif}
    DIV.center {text-align:center}
  </style>
  </head>
  
  <body>

<?php
  include("style/header.html");
?>

    
<h2>Machine Learning, Fall 2024</h2>

<h3>Course Description</h3>
<p>Machine learning allows computational systems to adaptively improve their performance with experience accumulated from the data observed. This course introduces the basics of learning theories, the design and analysis of learning algorithms, and some applications of machine learning.

<h3>People</h3>

<ul>
	  <li>instructor: <a href="http://www.csie.ntu.edu.tw/~htlin">Hsuan-Tien LIN</a> (htlin AT csie . ntu . edu . tw) [office hour: after classes, or by appointment]
<li>TAs and TA hour: html_ta AT csie . ntu . edu . tw
  <ul>
    <li>Yun-Ye Cai, Computer Science and Information Engineering
    <li>Chia-Wei Chang (VP of Project), Computer Science and Information Engineering
    <li>Shu-Han Chang, Physics
    <li>Chia-Le Chen, Atmospheric Science
    <li>Chu-Hsin Chen, Computer Science and Information Engineering
    <li>Chien-Yi (Chris) Chien (VP of Assignments), Computer Science and Information Engineering
    <li>Shih-Hsuan Chou, Physics
    <li>Mai Tan Ha (Head TA), Computer Science and Information Engineering
    <li>Bo-Kai Huang, Computer Science and Information Engineering
    <li>Chien-Jui Huang, Computer Science and Information Engineering
    <li>Hsiao-Chieh Kao, Engineering Science and Ocean Engineering
    <li>Yu-Wei Kuan, Computer Science and Information Engineering
    <li>Hsun-Yu (Yoyo) Lee, Information Management
    <li>I-Pei Lee, Electrical Engineering
    <li>Ren-Wei Liang, Computer Science and Information Engineering
    <li>Zhi-Bao Lu, Computer Science and Information Engineering      
    <li>Wei-Po Wang, Physics
    <li>Chun-Hao Yang, Computer Science and Information Engineering
  </ul>
</ul>
<h3>Course Information</h3>
<ul>

<li>Time: Mondays 13:20 to 16:20
<li>Room: CSIE R103 (with broadcast to R104, R105, R107 and screencast link below)
<li>NTU COOL: <a href="https://cool.ntu.edu.tw/courses/40495">https://cool.ntu.edu.tw/courses/40495</a>
<li>Synchronous Screencast: <a href="screencast.php">here</a>, containing slido interaction at #HTML2024FALL
<li>Textbook: <a href="http://amlbook.com"><i>Learning from Data</i></a>, by Yaser Abu-Mostafa, Malik Magdon-Ismail and Hsuan-Tien Lin
<li>Language: <b>English</b> teaching
<li>Grading: 70% homework, 30% project (tentative)
</ul>

<h3>Announcements</h3>
<ul>
  <li>2024/12/02: <a href="hw7">homework 7</a> announced, due on 12/16
  <li>2024/11/18: <a href="hw6">homework 6</a> announced, due on 12/02
  <li>2024/11/04: <a href="hw5">homework 5</a> announced, due on 11/18
  <li>2024/10/21: <a href="hw4">homework 4</a> announced, due on 11/04
  <li>2024/10/16: <a href="final">final project</a> announced, due on 12/23
  <li>2024/10/07: <a href="hw3">homework 3</a> announced, due on 10/21
  <li>2024/09/23: <a href="hw2">homework 2</a> announced, due on 10/07
  <li>2024/09/09: <a href="hw1">homework 1</a> announced, due on 10/07
  <li>2024/09/02: <a href="doc/hw0.pdf">homework 0</a> announced, due on 10/07
  <li>2024/08/13: course policy announced <a href="doc/policy.pdf">here</a>
</ul>

<h3>Class Policy</h3>
<ul>
  <li><a href="doc/policy.pdf">policy</a>
</ul>

<h3>Course Plan (tentative)</h3>

<TABLE border=1>
  <TR><TD>date</TD><TD>syllabus</TD><TD>todo/done</TD><TD>materials</TD></TR>
  <TR>
    <TD>09/02 (W1)</TD>
    <TD>
      course introduction;<br>
      <b>topic 1: when can machines learn?</b><br>
      the learning problem<br>      
    </TD>
    <TD><a href="doc/hw0.pdf">homework 0</a> announced</TD>
    <TD>
      <ul>
	<li><a href="https://youtube.com/live/_BxJdpI9Juk">recording from YouTube screencast</a>
	<li>slides for <a href="doc/00_handout.pdf">course introduction</a> (bug in p6/p12 fixed at 09/03 06:00)
	<li>slides for <a href="doc/01u_handout.pdf">the learning problem</a>; LFD 1.0, 1.1.1, 1.2.4 
     </ul>
    </TD>
  </TR>
  <TR>
    <TD>09/09 (W2)</TD>
    <TD>
      learning to answer yes/no;<br>
      types of learning      
    </TD>
    <TD>homework 1 announced</TD>
    <TD>
      <ul>
	<li><a href="https://youtube.com/live/ucD1pCEjSOI">recording from YouTube screencast</a>
	<li>slides for <a href="doc/02u_handout.pdf">learning to answer yes/no</a> (bug in p10 fixed at 09/16 08:30); LFD 1.1.2, 3.1
	<li>slides for <a href="doc/03u_handout.pdf">types of learning</a>; LFD 1.2; LFD 1.3
      </ul>
    </TD>
  </TR>   
  <TR>
    <TD>09/16 (W3)</TD>
    <TD>
      feasibility of learning;<br>
      <b>topic 2: why can machines learn?</b><br>
      training versus testing      
    </TD>
    <TD></TD>
    <TD>
      <ul>
	<li><a href="https://youtu.be/6Fh7D9SMhP8">recording from NTU CSIE (uploaded to YouTube)</a>---we are very sorry that there was an Internet issue and screencasting does not work that day
	<li>slides for <a href="doc/04u_handout.pdf">feasibility of learning</a>; LFD 1.3
	<li>suggested extended reading: <a href="https://direct.mit.edu/neco/article-abstract/8/7/1341/6016/The-Lack-of-A-Priori-Distinctions-Between-Learning">The Lack of A Priori Distinctions Between Learning Algorithms (Wolpert)</a>
	<li>slides for <a href="doc/05u_handout.pdf">training versus testing</a>; LFD 2.0, 2.1.1
      </ul>      
    </TD>
  </TR>
  <TR>
    <TD>09/23 (W4)</TD>
    <TD>
      (optional)theory of generalization;<br>
      the VC dimension;<br>
      noise and error      
    </TD>
    <TD>homework 2 announced</TD>
    <TD>
      <ul>
	<li><a href="https://youtube.com/live/3zbhL1Q7nu0">recording from YouTube screencast</a>
	<li>slides for <a href="doc/07u_handout.pdf">the VC dimension</a>; LFD 2.2
	<li>slides for <a href="doc/08u_handout.pdf">noise and error</a>; LFD 1.4
      </ul>
      optional:
      <ul>
	<li>slides for <a href="doc/06u_handout.pdf">theory of generalization<a/>; LFD 2.1.2
	<li><a href="https://www.youtube.com/watch?v=6FWRijsmLtE">English-teaching Lecture 6 by Prof. Yaser Abu-Mostafa at Caltech</a>
	<li>Mandarin-teaching Lecture 6 by Prof. Hsuan-Tien Lin at NTU: <a href="https://www.youtube.com/watch?v=rUFqB5Z3YHQ">part 1</a>, <a href="https://www.youtube.com/watch?v=OmRekto9rkc">part 2</a>, <a href="https://www.youtube.com/watch?v=6jtWUmaBqFU">part 3</a>, <a href="https://www.youtube.com/watch?v=GcxpsIvR7t8">part 4</a>
      </ul>
    </TD>  
  </TR>  
  <TR>
    <TD>09/30 (W5)</TD>
    <TD>
      <b>topic 3: how can machines learn?</b><br>
      linear regression;<br>
      logistic regression
    </TD>
    <TD></TD>
    <TD>
      <ul>
	<li><a href="https://youtube.com/live/cmBgRaFZHC8">recording from YouTube screencast</a>
	<li>slides for <a href="doc/09u_handout.pdf">linear regression</a>; LFD 3.2
	<li>slides for <a href="doc/10u_handout.pdf">logistic regression</a>; LFD 3.3
      </ul>      
    </TD>
  </TR>  
  <TR>
    <TD>10/07 (W6)</TD>
    <TD>
      linear models for classification;<br>
      nonlinear transformation
    </TD>
    <TD>homework 0 due; homework 1 due; homework 2 due; homework 3 announced</TD>
    <TD>
      <ul>
	<li><a href="https://youtube.com/live/V9MBd9Cie4g">recording from YouTube screencast</a>
	<li>slides for <a href="doc/11u_handout.pdf">linear models for classification</a>; LFD 3.3
	<li>slides for <a href="doc/12u_handout.pdf">nonlinear transformation</a>; LFD 3.4
      </ul>            
    </TD>
  </TR>
  <TR>
    <TD>10/14 (W7)</TD>
    <TD>
      <b>topic 4: how can machines learn better?</b><br>
      hazard of overfitting;<br>
      regularization
    </TD>
    <TD>
    </TD>
    <TD>
      <ul>
	<li><a href="https://youtube.com/live/MACbxDVfuys">recording from YouTube screencast</a>
	<li>slides for <a href="doc/13u_handout.pdf">hazard of overfitting</a>; LFD 4.0, 4.1
	<li>slides for <a href="doc/14u_handout.pdf">regularization</a>; LFD 4.2
      </ul>                  
    </TD>
  </TR>
  <TR>
    <TD>10/21 (W8)</TD>
    <TD>
      validation;<br>
      three learning principles
    </TD>
    <TD>homework 3 due; homework 4 announced; final project announced</TD>
    <TD>
      <ul>
	<li><a href="https://youtube.com/live/a09Ot4m657M">recording from YouTube screencast</a>
	<li>slides for <a href="doc/15u_handout.pdf">validation</a>; LFD 4.3
	<li>slides for <a href="doc/16u_handout.pdf">three learning principles</a>; LFD 5
      </ul>                        
    </TD>
  </TR>  
  <TR>
    <TD>10/28 (W9)</TD>
    <TD>
      <b>topic 5: how can machines learn by embedding numerous features?</b><br>
      linear support vector machine;<br>
      dual support vector machine
    </TD>
    <TD>
    </TD>
    <TD>
      <ul>
	<li><a href="https://youtube.com/live/nw_fUiS6l6A">recording from YouTube screencast</a>
	<li>slides for <a href="doc/201u_handout.pdf">linear support vector machine</a>; LFD e-8.1
	<li>slides for <a href="doc/202u_handout.pdf">dual support vector machine</a>; LFD e-8.2
      </ul>                        
    </TD>
  </TR>
  <TR>
    <TD>11/04 (W10)</TD>
    <TD>
      kernel support vector machine;<br>
      soft-margin support vector machine
    </TD>
    <TD>homework 4 due; homework 5 announced</TD>
    <TD>
      <ul>
	<li><a href="https://youtube.com/live/Dr9_wappIrM">recording from YouTube screencast</a>
	<li>slides for <a href="doc/203u_handout.pdf">kernel support vector machine</a>; LFD e-8.3
	<li>slides for <a href="doc/204u_handout.pdf">soft-margin support vector machine</a>; LFD e-8.4
      </ul>                        
    </TD>
  </TR>
  <TR>
    <TD>11/11 (W11)</TD>
    <TD>
      support vector machine for soft binary classification;<br>
      <b>topic 6: how can machines learn by combining predictive features?</b><br>
      blending and bagging
    </TD>
    <TD>
    </TD>
    <TD>
      <ul>
	<li><a href="https://youtube.com/live/x_JIP1HEqpo">recording from YouTube screencast</a>
	<li>slides for <a href="doc/205u_handout.pdf">support vector machine for soft binary classification</a>
	<li>suggested extended reading: <a href="http://www.csie.ntu.edu.tw/~htlin/paper/doc/plattprob.pdf">A Note on Platt's Probabilistic Outputs for Support Vector Machines (Lin, Weng and Lin)</a>
	<li>slides for <a href="doc/207u_handout.pdf">blending and bagging</a>
	<li>suggested extended reading: <a href="http://www.csie.ntu.edu.tw/~htlin/paper/doc/wskdd11cup_one.pdf">A linear ensemble of individual and blended models for music rating prediction (Chen et al.)</a>
	<li>suggested extended reading: <a href="http://statistics.berkeley.edu/sites/default/files/tech-reports/421.pdf">Bagging predictors (Breiman)</a>	  
      </ul>    
    </TD>
  </TR>
  <TR>
    <TD>11/18 (W12)</TD>
    <TD>
      adaptive boosting;<br>
      decision tree;<br>
      random forest;<br>      
      gradient boosted decision tree<br>
    </TD>
    <TD>
      homework 5 due; homework 6 announced
    </TD>
    <TD>
      <ul>
	<li><a href="https://youtube.com/live/FIFI4OpD6Qg">recording from YouTube screencast</a>
	<li>slides for <a href="doc/208u_handout.pdf">adaptive boosting</a>
	<li>suggested extended reading: <a href="http://cseweb.ucsd.edu/~yfreund/papers/IntroToBoosting.pdf">A short introduction to boosting (Freund and Schapire)</a>
	<li>slides for <a href="doc/209u_handout.pdf">decision tree</a>
	<li>suggested extended reading: <a href="http://www.stat.wisc.edu/~loh/treeprogs/guide/wires11.pdf">Classification and regression trees (overview of decision tree by Loh)</a>
	<li>suggested extended reading: <a href="http://www.amazon.com/Classification-Regression-Wadsworth-Statistics-Probability/dp/0412048418">Classification and regression trees (book of CART by Breiman et al.)</a>
	<li>slides for <a href="doc/210u_handout.pdf">random forest</a>
	<li>suggested extended reading: <a href="http://dx.doi.org/10.1023%2FA%3A1010933404324">Random forest (Breiman)</a>	  
	<li>slides for <a href="doc/211u_handout.pdf">gradient boosted decision tree</a>
	<li>suggested extended reading: <a href="http://www-stat.stanford.edu/~jhf/ftp/trebst.pdf">Greedy Function Approximation: A Gradient Boosting Machine (Friedman)</a>
      </ul>
    </TD>
  </TR>
  <TR>
    <TD>11/25 (W13)</TD>
    <TD>
      <b>topic 7: how can machines learn by distilling hidden features?</b><br>
      neural network;<br>
      deep learning
    </TD>
    <TD>
    </TD>
    <TD>
      <ul>
	<li><a href="https://youtube.com/live/OEwW_Hm2WaA">recording from YouTube screencast</a>
	<li>slides for <a href="doc/212u_handout.pdf">neural network</a>; LFD e-7.1, e-7.2, e-7.3, e-7.4 (selected parts)
	<li>slides for <a href="doc/213u_handout.pdf">deep learning</a>; LFD e-7.6
      </ul>
    </TD>
    <TD>
    </TD>
  </TR>
  <TR>
    <TD>12/02 (W14)</TD>
    <TD>
      modern deep learning
    <TD>
      homework 6 due; homework 7 announced
    </TD>
    <TD>
      <ul>
	<li><a href="https://youtube.com/live/27c1uzWcJns">recording from YouTube screencast</a>
	<li>slides for <a href="doc/302u_handout.pdf">deep learning activation</a>
	<li>ReLU: <a href="http://jmlr.org/proceedings/papers/v15/glorot11a/glorot11a.pdf">Deep sparse rectifier neural networks (Glorot, Bordes and Bengio)</a>
      <li>leaky ReLU: <a href="https://ai.stanford.edu/~amaas/papers/relu_hybrid_icml2013_final.pdf">Rectifier Nonlinearities Improve Neural Network Acoustic Models (Maas, Hannun and Ng)</a>
      <li>parametric ReLU: <a href="https://arxiv.org/abs/1502.01852">Delving Deep into Rectifiers: Surpassing Human-Level Performance on Image Net Classification (He, Zhang, Ren and Sun)</a>
      <li>slides for <a href="doc/303u_handout.pdf">deep learning optimization</a>
	<li>backprop and momentum: 
	  <a href="https://rdcu.be/b4ocH">Learning representations by back-propagating errors (Rumelhart, Hinton, and Williams)</a>;
	  <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.57.5612&amp;rep=rep1&amp;type=pdf">On the Momentum Term in Gradient Descent Learning Algorithms (Qian)</a>
	<li>adam: <a href="https://arxiv.org/pdf/1412.6980.pdf">Adam: A Method for Stochastic Optimization (Kingma and Ba)</a>
    </TD>
  </TR>
  <TR>
    <TD>12/09 (W15)</TD>
    <TD>      
      <b>no class as instructor needs to attend ACML 2024 and NeurIPS 2024</b>;<br>
    recording: machine learning for modern artificial intelligence
</TD>
    <TD>
    </TD>
    <TD>
    </TD>
  </TR>  
  <TR>
    <TD>12/16 (W16)</TD>
    <TD>finale
    </TD>
    <TD>homework 7 due</TD>
    <TD>
    </TD>
  </TR>  
  <TR>
    <TD>12/23 (W17)</TD>
    <TD><b>no class and winter vacation started (really?)</b></TD>
    <TD>final project due</TD>
    <TD></TD>
  </TR>
</TABLE>

<?php
  include("style/footer.html");
?>

</html>
